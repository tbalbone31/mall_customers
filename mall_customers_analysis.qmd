---
title: "Mall Customers Clustering Analysis"
author: "Your Name"
date: "May 23, 2025"
format: html
jupyter: true
---

# Introduction

This report analyzes the **Mall Customers dataset** using clustering techniques. The goal is to segment customers into distinct groups based on their spending behavior and income.

---

# Data Loading and Preprocessing

```{python}
# Load the dataset
import pandas as pd
from sklearn.preprocessing import StandardScaler
data = pd.read_csv('data/Mall_Customers.csv')

# Preprocess the data
cleaned_data = data.drop(columns=['CustomerID'])
cleaned_data = pd.get_dummies(cleaned_data, columns=['Genre'], drop_first=True)

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(cleaned_data)
```

---

# Determining the Optimal Number of Clusters

## Elbow Method

The Elbow Method helps determine the optimal number of clusters by plotting the inertia for different values of `k`.

```{python}
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Elbow Method
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)

# Plot the Elbow Method
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.show()
```

## Silhouette Score

The Silhouette Score evaluates the quality of clustering by measuring how similar each point is to its own cluster compared to other clusters.

```{python}
from sklearn.metrics import silhouette_score

# Silhouette Score
silhouette_scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_labels = kmeans.fit_predict(scaled_data)
    score = silhouette_score(scaled_data, cluster_labels)
    silhouette_scores.append(score)

# Plot the Silhouette Scores
plt.figure(figsize=(8, 5))
plt.plot(range(2, 11), silhouette_scores, marker='o', color='orange')
plt.title('Silhouette Score Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.show()
```

---

# Clustering and Evaluation

## Perform Clustering

```{python}
# Perform K-Means clustering
optimal_clusters = 6
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
clusters = kmeans.fit_predict(scaled_data)

# Add cluster labels to the original data
cleaned_data['Cluster'] = clusters
```

## Evaluate Clustering Quality

```{python}
from sklearn.metrics import davies_bouldin_score

# Calculate Silhouette Score
silhouette_avg = silhouette_score(scaled_data, clusters)
print(f"Silhouette Score: {silhouette_avg}")

# Calculate Davies-Bouldin Index
davies_bouldin = davies_bouldin_score(scaled_data, clusters)
print(f"Davies-Bouldin Index: {davies_bouldin}")
```

---

# Visualization

## Visualize Clusters Using Original Features

```{python}
import seaborn as sns

# Visualize clusters
plt.figure(figsize=(8, 5))
sns.scatterplot(
    x=cleaned_data['Annual Income (k$)'],
    y=cleaned_data['Spending Score (1-100)'],
    hue=clusters,
    palette='tab10'
)
plt.title('Clusters Visualization (Original Features)')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend(title='Cluster')
plt.show()
```

## Visualize Clusters with Centroids

```{python}
# Add centroids (convert from scaled space to original space)
centroids_original = scaler.inverse_transform(kmeans.cluster_centers_)

# Visualize clusters with centroids
plt.figure(figsize=(8, 5))
sns.scatterplot(
    x=cleaned_data['Annual Income (k$)'],
    y=cleaned_data['Spending Score (1-100)'],
    hue=clusters,
    palette='tab10'
)

# Plot centroids
plt.scatter(
    centroids_original[:, cleaned_data.columns.get_loc('Annual Income (k$)')],
    centroids_original[:, cleaned_data.columns.get_loc('Spending Score (1-100)')],
    s=200, c='red', label='Centroids', marker='X'
)

# Add labels to centroids with an offset
dx, dy = 5, 5
for i, (x, y) in enumerate(zip(
    centroids_original[:, cleaned_data.columns.get_loc('Annual Income (k$)')],
    centroids_original[:, cleaned_data.columns.get_loc('Spending Score (1-100)')]
)):
    plt.text(x + dx, y + dy, f'Cluster {i}', color='black', fontsize=10, ha='center', va='center')

plt.title('Clusters Visualization with Centroids (Original Features)')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend(title='Cluster')
plt.show()
```

## Visualize Clusters Using PCA

```{python}
from sklearn.decomposition import PCA

# PCA for visualization
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)

# Plot PCA clusters
plt.figure(figsize=(8, 5))
sns.scatterplot(
    x=pca_data[:, 0],
    y=pca_data[:, 1],
    hue=clusters,
    palette='viridis'
)

# Add centroids in PCA space
pca_centroids = pca.transform(kmeans.cluster_centers_)
plt.scatter(
    pca_centroids[:, 0],
    pca_centroids[:, 1],
    s=200, c='red', label='Centroids', marker='X'
)

plt.title('Clusters Visualization (PCA)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title='Cluster')
plt.show()
```

---

# Cluster Interpretation

Based on the clustering results:
- **Cluster 0**: High-income, high-spending customers (e.g., "High-Value Customers").
- **Cluster 1**: Low-income, low-spending customers (e.g., "Budget-Conscious Customers").
- **Cluster 2**: Moderate-income, moderate-spending customers.
- **Cluster 3**: High-income, low-spending customers (e.g., "Potential High-Value Customers").
- **Cluster 4**: Low-income, high-spending customers (e.g., "Deal Seekers").
- **Cluster 5**: Other.

---

# Conclusion

This analysis segmented customers into six distinct groups based on their income and spending behavior. These insights can help businesses tailor marketing strategies, improve customer engagement, and optimize resource allocation. Future work could involve incorporating additional features or testing other clustering algorithms for further refinement.